2009-07-22T00:40:00.000Z	blackzpeez		hello, how can I download an entire site (wiki-style) using wget?
2009-07-22T00:40:00.000Z	jrib	blackzpeez	check the recursive and depth switches in man wget
2009-07-22T00:41:00.000Z	blackzpeez	jrib	yes I've RTFM but they don't work
2009-07-22T00:41:00.000Z	jrib	blackzpeez	"don't work"?  Pastebin your command and the output.
2009-07-22T00:41:00.000Z	blackzpeez		with wiki.debian.org/Development
2009-07-22T00:41:00.000Z	blackzpeez		no it doesn't download all I want to
2009-07-22T00:41:00.000Z	blackzpeez		of course the command works
2009-07-22T00:47:00.000Z	blackzpeez	jrib	http://paste.ubuntu.com/223816/
2009-07-22T00:48:00.000Z	blackzpeez	jrib	plz take a look
2009-07-22T00:49:00.000Z	blackzpeez	jrib	so???
2009-07-22T00:50:00.000Z	jrib	blackzpeez	patience... Why is your -l0?
2009-07-22T00:50:00.000Z	blackzpeez		recursion level => infinite check the man jejejej
2009-07-22T00:50:00.000Z	blackzpeez		kidding
2009-07-22T00:50:00.000Z	blackzpeez		recursion -> infinite
2009-07-22T00:53:00.000Z	blackzpeez	jrib	there's no way I'm going to download the entire wiki, I'm only interested in the links in that page
2009-07-22T00:54:00.000Z	jrib	blackzpeez	so you probably don't want -l0 then
2009-07-22T00:54:00.000Z	blackzpeez	jrib	i removed it, still the same, only that page
2009-07-22T00:59:00.000Z	jrib	blackzpeez	don't know
2009-07-22T00:59:00.000Z	blackzpeez	jrib	nevermind I'll PYscript it ! ;)
2009-07-22T01:02:00.000Z	jrib	blackzpeez	robots.txt is at fault
2009-07-22T01:03:00.000Z	blackzpeez	jrib	there's anything I can do
2009-07-22T01:03:00.000Z	jrib	blackzpeez	http://wget.addictivecode.org/FrequentlyAskedQuestions#head-badfdf9c2571452db5d048ff7e080a9247cf6b97
